# -*- coding: utf-8 -*-
"""
Fine-tune DistilBERT for 3-class sentiment classification
with simple regex cleaning (no NLTK required)
"""

import re
import string
import pandas as pd
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
import torch
from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix
)
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline
)
from tqdm import tqdm

# 1. Precompute stopwords
stop_words = set(ENGLISH_STOP_WORDS)

# 2. Define a lightweight cleaning/tokenization function
def clean_text(text):
    # a) Lowercase
    text = text.lower()
    # b) Remove URLs
    text = re.sub(r"http\S+|www\.\S+", "", text)
    # c) Remove markdown links ([text](url))
    text = re.sub(r"\[.*?\]\(.*?\)", "", text)
    # d) Remove Reddit user references
    text = re.sub(r"/?u/\w+", "", text)
    # e) Remove HTML tags
    text = re.sub(r"<.*?>", "", text)
    # f) Remove digits and punctuation
    text = re.sub(r"\d+", "", text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    # g) Tokenize on whitespace & non-word boundaries
    tokens = re.findall(r"\b[a-z]+\b", text)
    # h) Remove stopwords
    tokens = [t for t in tokens if t not in stop_words]
    return " ".join(tokens)

# 3. Load your labeled sample (must have 'comment_body' & 'sentiment')
df = pd.read_excel("sample_labeled.xlsx", engine="openpyxl")
df["cleaned_body"] = df["comment_body"].apply(clean_text)

# 4. Map sentiments to integers
label2id = {"negative": 0, "neutral": 1, "positive": 2}
df["label"] = df["sentiment"].map(label2id)

# 5. Stratified train/test split
train_df, test_df = train_test_split(
    df, test_size=0.2, stratify=df["label"], random_state=42
)

# 6. Oversample minority classes in training set
max_count = train_df["label"].value_counts().max()
oversampled = []
for lbl, grp in train_df.groupby("label"):
    oversampled.append(grp.sample(n=max_count, replace=True, random_state=42))
train_bal = pd.concat(oversampled).sample(frac=1, random_state=42)

# 7. Prepare Transformer tokenizer
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 8. PyTorch Dataset wrapper
class SentimentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

# 9. Create train/eval datasets using cleaned text
train_dataset = SentimentDataset(
    train_bal["cleaned_body"], train_bal["label"], tokenizer
)
eval_dataset = SentimentDataset(
    test_df["cleaned_body"], test_df["label"], tokenizer
)

# 10. Load pretrained DistilBERT for 3‐class classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, num_labels=3
)

# 11. Define training arguments (matching eval/save strategy)
training_args = TrainingArguments(
    output_dir="hf-sentiment-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=5,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    logging_dir="logs",
    logging_steps=20,
    do_train=True,
    do_eval=True,
    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=1,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,
)

# 12. Metrics computation callback
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro"),
    }

# 13. Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# 14. Train the model
trainer.train()
import numpy as np
from scipy.special import softmax
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix

# 1) Run the model on eval_dataset to get raw logits + true labels
pred_out = trainer.predict(eval_dataset)
logits   = pred_out.predictions      # shape (N,3)
labels   = pred_out.label_ids

# 2) Convert to softmax probs
probs = softmax(logits, axis=1)       # shape (N,3)

# 3) Grid-search negative threshold
neg_thrs = np.linspace(0.1, 0.5, 17)   # e.g. 0.1 → 0.5 by 0.025
best = {'f1': -1, 'neg_thr': None}

for thr in neg_thrs:
    # apply threshold rule
    preds = []
    for p in probs:
        if p[0] >= thr:
            preds.append(0)
        else:
            # pick between neutral(1) and positive(2)
            preds.append(1 + np.argmax(p[1:3]))
    f1 = f1_score(labels, preds, average='macro', labels=[0,1,2])
    if f1 > best['f1']:
        best.update({'f1': f1, 'neg_thr': thr})

print(f"Best negative threshold: {best['neg_thr']:.3f} → macro-F1 {best['f1']:.3f}")

# 4) Final predictions on eval set with tuned thr
final_preds = []
for p in probs:
    if p[0] >= best['neg_thr']:
        final_preds.append(0)
    else:
        final_preds.append(1 + np.argmax(p[1:3]))

# 5) Evaluation
print("Accuracy:", accuracy_score(labels, final_preds))
print("Classification Report:")
print(classification_report(labels, final_preds, target_names=['negative','neutral','positive']))
print("Confusion Matrix:")
print(confusion_matrix(labels, final_preds, labels=[0,1,2]))

# 6) Inference on full dataset
df_full = pd.read_csv('Pirates 2 .csv')
# tokenize full
enc = tokenizer(df_full['comment_body'].tolist(),
                padding='max_length', truncation=True,
                max_length=128, return_tensors='pt')
with torch.no_grad():
    logits_full = model(**enc).logits.cpu().numpy()
probs_full = softmax(logits_full, axis=1)

def predict_with_neg_thr(p):
    if p[0] >= best['neg_thr']:
        return 'negative'
    return ['neutral','positive'][np.argmax(p[1:3])]

df_full['sentiment'] = [predict_with_neg_thr(p) for p in probs_full]
df_full.to_csv('Pirates_with_custom_sentiment.csv', index=False)
print("Done – applied negative threshold rule.")
